{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Layer\nimport numpy as np\nimport math\n\nclass Word2VecKeras(object):\n    '''\n    Using keras functional API to implement word embedding\n    '''\n    def __init__(self, vocab_size=0, embedding_dim=16, batch_size=None, epochs=10):\n        self.vocab_size = vocab_size\n        self.embedding_dim = embedding_dim\n        self.epochs = epochs\n        self.batch_size = batch_size\n            \n    def build(self):\n        input_target = keras.Input(shape=(1, ), name=\"target_word\")  \n        input_context = keras.Input(shape=(1,), name=\"context_word\")  \n        embedding = keras.layers.Embedding(self.vocab_size, self.embedding_dim, input_length=1, name='embedding')\n        target = embedding(input_target)\n        target = keras.layers.Reshape((self.embedding_dim, 1))(target)\n        context = embedding(input_context)\n        context = keras.layers.Reshape((self.embedding_dim, 1))(context)\n        # to compute the cosine similarity between two vectors, set normalize=True\n        dot_product = keras.layers.Dot(axes=1, normalize=False)([target,context])\n        dot_product = keras.layers.Reshape((1,))(dot_product)\n        output = keras.layers.Dense(1, activation='sigmoid')(dot_product)\n        \n        model = keras.Model(inputs = [input_target, input_context], outputs = output)\n        model.compile(optimizer=keras.optimizers.RMSprop(1e-3), metrics=['accuracy'],\n                      loss=keras.losses.BinaryCrossentropy(from_logits=False),)\n        \n        model.summary()\n        return model    \n    \n    def train(self, x_target, x_context, y, model):\n        '''\n        Parameters\n        ----------\n        x_target : (vocab_size, 1) tensor target word ID \n            example: [[1],[2],...] \n        x_context : (vocab_size, 1) tensor context word ID\n            example: [[1],[2],...]\n        y : 0 or 1 (vocab_size,)label vector\n            example: [1,0,0,1,...]\n        model : keras model object\n\n        Returns\n        -------\n        (vocab_size, embedding_dim) tensor\n        '''\n        hist = model.fit( {\"target_word\": x_target, \"context_word\": x_context}, y,\n                  self.batch_size, self.epochs,)\n        \n        return {\"embedding\": model.get_layer('embedding').get_weights()[0], \n                \"acc\": hist.history['accuracy'][-1]}\nif __name__=='__main__':\n    '''\n    Testing examples\n    '''\n    corpus_raw = 'He is the king . The king is royal . She is the royal queen '\n    # raw sentences is a list of sentences.\n    raw_sentences = corpus_raw.split('.')\n    sentences = []\n    for sentence in raw_sentences:\n        sentences.append(sentence.split())\n\n    data = []\n    WINDOW_SIZE = 2\n    for sentence in sentences:\n        for word_index, word in enumerate(sentence):\n            for nb_word in sentence[max(word_index - WINDOW_SIZE, 0) :\n                                    min(word_index + WINDOW_SIZE, len(sentence)) + 1] : \n                if nb_word != word:\n                    data.append([word, nb_word])\n    print(data)\n    words = []\n    for word in corpus_raw.split():\n        if word != '.': # because we don't want to treat . as a word\n            words.append(word)\n    words = set(words) # so that all duplicate words are removed\n    word2int = {}\n    int2word = {}\n    vocab_size = len(words) # gives the total number of unique words\n    for i,word in enumerate(words):\n        word2int[word] = i\n        int2word[i] = word\n    \n   \n    '''\n    #try Word2VecKeras\n    '''\n    # negative sampling\n    import random\n    num_sampled = 2\n    data = list({tuple(item) for item in map(sorted, data)})\n    wp = [(word2int[p[0]], word2int[p[1]]) for p in data]\n    sample = [(p[0], p[1], 1) for p in wp]\n\n    wlst = list(int2word.keys())\n    for k in int2word.keys():\n        random.shuffle(wlst)    \n        counter = 0\n        ind = 0\n        for s in wlst:\n            pair = (k, s)\n            if pair not in wp and (pair[1], pair[0]) not in wp and k != s:\n                sample.append((k, s, 0))\n                wp.append((k,s))\n                counter += 1\n            if counter >= 3:\n                break \n    sample = np.array(sample, dtype='int64')\n    \n    x_context = sample[:,0]\n    x_target = sample[:,1]\n    xc = x_context.reshape(len(x_context), 1)\n    xt = x_target.reshape(len(x_target), 1)\n    xc = tf.convert_to_tensor(xc)\n    xt = tf.convert_to_tensor(xt)\n    y = sample[:,2]\n    y = tf.convert_to_tensor(y)\n    obj = Word2VecKeras(vocab_size=vocab_size, embedding_dim=5, batch_size = 10, epochs=100)\n    m = obj.build()\n    embed_mat = obj.train(xc, xt, y, m)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-25T18:06:05.866928Z","iopub.execute_input":"2023-04-25T18:06:05.867515Z","iopub.status.idle":"2023-04-25T18:06:08.846005Z","shell.execute_reply.started":"2023-04-25T18:06:05.867464Z","shell.execute_reply":"2023-04-25T18:06:08.844723Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[['He', 'is'], ['He', 'the'], ['is', 'He'], ['is', 'the'], ['is', 'king'], ['the', 'He'], ['the', 'is'], ['the', 'king'], ['king', 'is'], ['king', 'the'], ['The', 'king'], ['The', 'is'], ['king', 'The'], ['king', 'is'], ['king', 'royal'], ['is', 'The'], ['is', 'king'], ['is', 'royal'], ['royal', 'king'], ['royal', 'is'], ['She', 'is'], ['She', 'the'], ['is', 'She'], ['is', 'the'], ['is', 'royal'], ['the', 'She'], ['the', 'is'], ['the', 'royal'], ['the', 'queen'], ['royal', 'is'], ['royal', 'the'], ['royal', 'queen'], ['queen', 'the'], ['queen', 'royal']]\nModel: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n target_word (InputLayer)       [(None, 1)]          0           []                               \n                                                                                                  \n context_word (InputLayer)      [(None, 1)]          0           []                               \n                                                                                                  \n embedding (Embedding)          (None, 1, 5)         40          ['target_word[0][0]',            \n                                                                  'context_word[0][0]']           \n                                                                                                  \n reshape (Reshape)              (None, 5, 1)         0           ['embedding[0][0]']              \n                                                                                                  \n reshape_1 (Reshape)            (None, 5, 1)         0           ['embedding[1][0]']              \n                                                                                                  \n dot (Dot)                      (None, 1, 1)         0           ['reshape[0][0]',                \n                                                                  'reshape_1[0][0]']              \n                                                                                                  \n reshape_2 (Reshape)            (None, 1)            0           ['dot[0][0]']                    \n                                                                                                  \n dense (Dense)                  (None, 1)            2           ['reshape_2[0][0]']              \n                                                                                                  \n==================================================================================================\nTotal params: 42\nTrainable params: 42\nNon-trainable params: 0\n__________________________________________________________________________________________________\nEpoch 1/100\n3/3 [==============================] - 1s 7ms/step - loss: 0.6934 - accuracy: 0.4444\nEpoch 2/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6929 - accuracy: 0.5556\nEpoch 3/100\n3/3 [==============================] - 0s 3ms/step - loss: 0.6929 - accuracy: 0.5556\nEpoch 4/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6926 - accuracy: 0.5185\nEpoch 5/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6925 - accuracy: 0.6296\nEpoch 6/100\n3/3 [==============================] - 0s 3ms/step - loss: 0.6923 - accuracy: 0.6296\nEpoch 7/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6923 - accuracy: 0.6296\nEpoch 8/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6920 - accuracy: 0.7037\nEpoch 9/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6919 - accuracy: 0.6667\nEpoch 10/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6917 - accuracy: 0.6667\nEpoch 11/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6914 - accuracy: 0.7407\nEpoch 12/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6913 - accuracy: 0.7407\nEpoch 13/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.6911 - accuracy: 0.8148\nEpoch 14/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6909 - accuracy: 0.8148\nEpoch 15/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6906 - accuracy: 0.8148\nEpoch 16/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6904 - accuracy: 0.7778\nEpoch 17/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6902 - accuracy: 0.8148\nEpoch 18/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6900 - accuracy: 0.8148\nEpoch 19/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6897 - accuracy: 0.7778\nEpoch 20/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.6895 - accuracy: 0.7778\nEpoch 21/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6891 - accuracy: 0.7778\nEpoch 22/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6888 - accuracy: 0.7778\nEpoch 23/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6886 - accuracy: 0.7407\nEpoch 24/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6882 - accuracy: 0.7778\nEpoch 25/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6879 - accuracy: 0.7778\nEpoch 26/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6876 - accuracy: 0.8148\nEpoch 27/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6872 - accuracy: 0.8148\nEpoch 28/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6868 - accuracy: 0.7778\nEpoch 29/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6864 - accuracy: 0.8519\nEpoch 30/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6862 - accuracy: 0.7407\nEpoch 31/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6858 - accuracy: 0.8519\nEpoch 32/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6853 - accuracy: 0.8889\nEpoch 33/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6849 - accuracy: 0.8519\nEpoch 34/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6844 - accuracy: 0.8889\nEpoch 35/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6840 - accuracy: 0.8889\nEpoch 36/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6835 - accuracy: 0.8889\nEpoch 37/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6831 - accuracy: 0.8889\nEpoch 38/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6826 - accuracy: 0.8889\nEpoch 39/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6822 - accuracy: 0.8889\nEpoch 40/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6816 - accuracy: 0.8889\nEpoch 41/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6812 - accuracy: 0.8889\nEpoch 42/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6807 - accuracy: 0.8889\nEpoch 43/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6802 - accuracy: 0.8889\nEpoch 44/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6797 - accuracy: 0.8889\nEpoch 45/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6791 - accuracy: 0.8519\nEpoch 46/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.6785 - accuracy: 0.8889\nEpoch 47/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.6780 - accuracy: 0.8889\nEpoch 48/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6774 - accuracy: 0.8889\nEpoch 49/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6768 - accuracy: 0.8889\nEpoch 50/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6764 - accuracy: 0.8889\nEpoch 51/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6756 - accuracy: 0.8889\nEpoch 52/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6751 - accuracy: 0.8889\nEpoch 53/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.6744 - accuracy: 0.8519\nEpoch 54/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6738 - accuracy: 0.8889\nEpoch 55/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6730 - accuracy: 0.8889\nEpoch 56/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6724 - accuracy: 0.8889\nEpoch 57/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6717 - accuracy: 0.8889\nEpoch 58/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6710 - accuracy: 0.8889\nEpoch 59/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6704 - accuracy: 0.8889\nEpoch 60/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6696 - accuracy: 0.8519\nEpoch 61/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6689 - accuracy: 0.8519\nEpoch 62/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6683 - accuracy: 0.8148\nEpoch 63/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6675 - accuracy: 0.8519\nEpoch 64/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6668 - accuracy: 0.8148\nEpoch 65/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6659 - accuracy: 0.8519\nEpoch 66/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6651 - accuracy: 0.8519\nEpoch 67/100\n3/3 [==============================] - 0s 3ms/step - loss: 0.6644 - accuracy: 0.8148\nEpoch 68/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6636 - accuracy: 0.8148\nEpoch 69/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6628 - accuracy: 0.8519\nEpoch 70/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6620 - accuracy: 0.8519\nEpoch 71/100\n3/3 [==============================] - 0s 3ms/step - loss: 0.6612 - accuracy: 0.8519\nEpoch 72/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6603 - accuracy: 0.8519\nEpoch 73/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6595 - accuracy: 0.8519\nEpoch 74/100\n3/3 [==============================] - 0s 3ms/step - loss: 0.6585 - accuracy: 0.8148\nEpoch 75/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6576 - accuracy: 0.8519\nEpoch 76/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6567 - accuracy: 0.8519\nEpoch 77/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6559 - accuracy: 0.8519\nEpoch 78/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6550 - accuracy: 0.8519\nEpoch 79/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6540 - accuracy: 0.8519\nEpoch 80/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6532 - accuracy: 0.8519\nEpoch 81/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6522 - accuracy: 0.8519\nEpoch 82/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6514 - accuracy: 0.8148\nEpoch 83/100\n3/3 [==============================] - 0s 3ms/step - loss: 0.6504 - accuracy: 0.8148\nEpoch 84/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6494 - accuracy: 0.8148\nEpoch 85/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6485 - accuracy: 0.8148\nEpoch 86/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6475 - accuracy: 0.8148\nEpoch 87/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6465 - accuracy: 0.8148\nEpoch 88/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6455 - accuracy: 0.8148\nEpoch 89/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6445 - accuracy: 0.8148\nEpoch 90/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6436 - accuracy: 0.8148\nEpoch 91/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6426 - accuracy: 0.8148\nEpoch 92/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6415 - accuracy: 0.8148\nEpoch 93/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6404 - accuracy: 0.8148\nEpoch 94/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6395 - accuracy: 0.8148\nEpoch 95/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6386 - accuracy: 0.8148\nEpoch 96/100\n3/3 [==============================] - 0s 5ms/step - loss: 0.6374 - accuracy: 0.8148\nEpoch 97/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6363 - accuracy: 0.8148\nEpoch 98/100\n3/3 [==============================] - 0s 3ms/step - loss: 0.6353 - accuracy: 0.8148\nEpoch 99/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6341 - accuracy: 0.8148\nEpoch 100/100\n3/3 [==============================] - 0s 4ms/step - loss: 0.6330 - accuracy: 0.8148\n","output_type":"stream"}]},{"cell_type":"code","source":" word2int","metadata":{"execution":{"iopub.status.busy":"2023-04-25T18:13:18.668331Z","iopub.execute_input":"2023-04-25T18:13:18.669687Z","iopub.status.idle":"2023-04-25T18:13:18.678832Z","shell.execute_reply.started":"2023-04-25T18:13:18.669632Z","shell.execute_reply":"2023-04-25T18:13:18.676634Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'queen': 0,\n 'She': 1,\n 'is': 2,\n 'royal': 3,\n 'He': 4,\n 'king': 5,\n 'the': 6,\n 'The': 7}"},"metadata":{}}]},{"cell_type":"code","source":"embed_mat","metadata":{"execution":{"iopub.status.busy":"2023-04-25T18:11:18.070395Z","iopub.execute_input":"2023-04-25T18:11:18.070930Z","iopub.status.idle":"2023-04-25T18:11:18.081030Z","shell.execute_reply.started":"2023-04-25T18:11:18.070892Z","shell.execute_reply":"2023-04-25T18:11:18.079523Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'embedding': array([[ 0.25452635,  0.3243817 ,  0.22087796, -0.03983046, -0.15438262],\n        [ 0.2846581 ,  0.20850565,  0.19028811, -0.04843751, -0.00486256],\n        [-0.07590873, -0.08779476,  0.05023467,  0.1651302 ,  0.31930658],\n        [ 0.02757882,  0.14187321,  0.24643363,  0.17486526,  0.16142657],\n        [ 0.01674843, -0.099883  , -0.06969719, -0.21033859,  0.10611706],\n        [-0.23370776, -0.1295089 ,  0.0238382 ,  0.2599329 ,  0.26343933],\n        [ 0.19322447,  0.20150357,  0.28032136,  0.04269268,  0.25422463],\n        [-0.28647092, -0.2433225 , -0.27937084,  0.21386106,  0.08995689]],\n       dtype=float32),\n 'acc': 0.8148148059844971}"},"metadata":{}}]}]}